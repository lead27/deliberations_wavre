import requests
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime

# Configuration
URL_BASE = "https://www.deliberations.be/wavre/decisions"
DELAI_ENTRE_REQUETES = 3  # secondes entre chaque requ√™te pour √©viter de surcharger le serveur

def detecter_seance_la_plus_recente():
    """
    Cette fonction d√©tecte automatiquement la s√©ance la plus r√©cente
    en analysant la premi√®re page des d√©cisions
    """
    print("D√©tection de la s√©ance la plus r√©cente...")
    
    response = requests.get(URL_BASE)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Chercher le s√©lecteur de s√©ance
    select_seance = soup.find('select', {'id': 'seance'})
    
    if select_seance:
        # Trouver l'option s√©lectionn√©e (selected="selected")
        option_selectionnee = select_seance.find('option', {'selected': 'selected'})
        if option_selectionnee:
            seance_id = option_selectionnee.get('value')
            seance_nom = option_selectionnee.get('title')
            print(f"S√©ance d√©tect√©e : {seance_nom}")
            print(f"ID : {seance_id}\n")
            return seance_id, seance_nom
    
    print("Impossible de d√©tecter la s√©ance. Utilisation de la premi√®re carte trouv√©e...\n")
    return None, None

def extraire_liens_deliberations(seance_id):
    """
    √âtape 1 : Cette fonction va parcourir TOUTES les pages
    d'une s√©ance sp√©cifique et r√©cup√®re tous les liens
    """
    print("üì• Analyse de la pagination...")
    
    tous_les_liens = []
    liens_vus = set()  # Pour √©viter les doublons
    page_actuelle = 0
    pages_vides_consecutives = 0
    
    while True:
        # Construction de l'URL avec la s√©ance et la pagination
        if page_actuelle == 0:
            url = f"{URL_BASE}?seance={seance_id}"
        else:
            url = f"{URL_BASE}?seance={seance_id}&b_start:int={page_actuelle}"
        
        print(f"  üìÑ Page {page_actuelle // 20 + 1}...")
        
        try:
            # On fait une demande pour r√©cup√©rer la page web
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # On cherche toutes les cartes de d√©lib√©rations
            cartes = soup.find_all('div', class_='item-card')
            
            if not cartes:
                # Plus de cartes = on a fini
                print("  ‚ö†Ô∏è  Aucune carte trouv√©e sur cette page")
                break
            
            nouveaux_liens = 0
            for carte in cartes:
                # Dans chaque carte, on cherche le lien
                lien_element = carte.find('a', class_='filled-link')
                if lien_element and lien_element.get('href'):
                    # On r√©cup√®re l'URL compl√®te
                    url_complete = lien_element['href']
                    if not url_complete.startswith('http'):
                        url_complete = 'https://www.deliberations.be' + url_complete
                    
                    # On v√©rifie qu'on n'a pas d√©j√† ce lien
                    if url_complete not in liens_vus:
                        tous_les_liens.append(url_complete)
                        liens_vus.add(url_complete)
                        nouveaux_liens += 1
            
            print(f"     ‚Üí {nouveaux_liens} nouvelles d√©lib√©rations trouv√©es")
            
            # Si on n'a trouv√© aucun nouveau lien, on arr√™te
            if nouveaux_liens == 0:
                pages_vides_consecutives += 1
                if pages_vides_consecutives >= 2:
                    print("  ‚úì Plus de nouvelles d√©lib√©rations")
                    break
            else:
                pages_vides_consecutives = 0
            
            # Passer √† la page suivante
            page_actuelle += 20
            time.sleep(1)  # Petite pause entre les pages
            
        except Exception as e:
            print(f"  ‚ùå Erreur sur cette page: {e}")
            break
    
    print(f"\n‚úÖ Trouv√© {len(tous_les_liens)} d√©lib√©rations au total\n")
    return tous_les_liens

def extraire_contenu_deliberation(url):
    """
    √âtape 2 : Cette fonction va chercher le contenu d√©taill√©
    d'une d√©lib√©ration sp√©cifique
    """
    print(f"üìÑ Extraction de : {url.split('/')[-1][:50]}...")
    
    # On attend un peu pour ne pas surcharger le serveur
    time.sleep(DELAI_ENTRE_REQUETES)
    
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # On r√©cup√®re le titre
        titre = soup.find('h1')
        titre_texte = titre.get_text(strip=True) if titre else "Titre non trouv√©"
        
        # On r√©cup√®re tout le contenu principal
        contenu = soup.find('article', id='content')
        if contenu:
            # On enl√®ve les balises HTML pour garder juste le texte
            texte = contenu.get_text(separator='\n', strip=True)
        else:
            texte = "Contenu non trouv√©"
        
        print(f"‚úÖ Extraction r√©ussie\n")
        
        return {
            'url': url,
            'titre': titre_texte,
            'contenu': texte
        }
    
    except Exception as e:
        print(f"‚ùå Erreur : {e}\n")
        return {
            'url': url,
            'titre': 'Erreur',
            'contenu': f'Impossible d\'extraire le contenu : {e}'
        }

def sauvegarder_resultats(deliberations, seance_id=None, seance_nom=None, nom_fichier='deliberations_wavre.json'):
    """
    √âtape 3 : Cette fonction sauvegarde tous les r√©sultats
    dans un fichier JSON (format facile √† lire) avec des m√©tadonn√©es
    """
    print(f"üíæ Sauvegarde des r√©sultats dans {nom_fichier}...")

    export = {
        "exported_at": datetime.utcnow().replace(microsecond=0).isoformat() + "Z",
        "seance": {
            "id": seance_id,
            "nom": seance_nom
        },
        "deliberations": deliberations
    }

    with open(nom_fichier, 'w', encoding='utf-8') as f:
        json.dump(export, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ Sauvegarde termin√©e !\n")

def creer_resume_texte(deliberations, nom_fichier, seance_nom):
    """
    √âtape 4 : Cette fonction cr√©e un fichier texte facile √† lire
    avec toutes les d√©lib√©rations
    """
    print(f"üìù Cr√©ation du r√©sum√© en texte dans {nom_fichier}...")
    
    with open(nom_fichier, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("D√âLIB√âRATIONS DU CONSEIL COMMUNAL DE WAVRE\n")
        if seance_nom:
            f.write(f"{seance_nom}\n")
        f.write("=" * 80 + "\n\n")
        
        for i, delib in enumerate(deliberations, 1):
            f.write(f"\n{'='*80}\n")
            f.write(f"POINT {i}\n")
            f.write(f"{'='*80}\n\n")
            f.write(f"TITRE : {delib['titre']}\n\n")
            f.write(f"URL : {delib['url']}\n\n")
            f.write(f"CONTENU :\n{'-'*80}\n")
            f.write(delib['contenu'])
            f.write(f"\n\n")
    
    print(f"‚úÖ R√©sum√© cr√©√© !\n")

# ===== PROGRAMME PRINCIPAL =====
def main():
    """
    C'est ici que tout commence !
    """
    print("\n" + "="*80)
    print("EXTRACTION DES D√âLIB√âRATIONS DU CONSEIL COMMUNAL DE WAVRE")
    print("="*80 + "\n")
    
    # √âtape 0 : D√©tecter la s√©ance la plus r√©cente
    seance_id, seance_nom = detecter_seance_la_plus_recente()
    
    if not seance_id:
        print("Erreur : impossible de d√©tecter la s√©ance.")
        return
    
    # √âtape 1 : R√©cup√©rer tous les liens de cette s√©ance
    liens = extraire_liens_deliberations(seance_id)
    
    if not liens:
        print("Aucune d√©lib√©ration trouv√©e. V√©rifiez l'URL.")
        return
    
    # √âtape 2 : Extraire le contenu de chaque d√©lib√©ration
    print(f"D√©but de l'extraction de {len(liens)} d√©lib√©rations...")
    print(f"Temps estim√© : environ {len(liens) * DELAI_ENTRE_REQUETES // 60} minutes\n")
    
    deliberations = []
    for i, lien in enumerate(liens, 1):
        print(f"[{i}/{len(liens)}]")
        delib = extraire_contenu_deliberation(lien)
        deliberations.append(delib)
    
    # √âtape 3 : Sauvegarder les r√©sultats
    sauvegarder_resultats(deliberations, seance_id, seance_nom)
    creer_resume_texte(deliberations, 'resume_deliberations.txt', seance_nom)
    
    print("\n" + "="*80)
    print("EXTRACTION TERMIN√âE !")
    print("="*80)
    print(f"\nVous pouvez consulter :")
    print(f"  - deliberations_wavre.json (format structur√©)")
    print(f"  - resume_deliberations.txt (format texte lisible)")
    print("\nCes fichiers sont dans le m√™me dossier que votre script.\n")

# Point d'entr√©e du programme
if __name__ == "__main__":
    main()
